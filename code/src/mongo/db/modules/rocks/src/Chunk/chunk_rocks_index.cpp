/**
 *    Copyright (C) 2014 MongoDB Inc.
 *
 *    This program is free software: you can redistribute it and/or  modify
 *    it under the terms of the GNU Affero General Public License, version 3,
 *    as published by the Free Software Foundation.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU Affero General Public License for more details.
 *
 *    You should have received a copy of the GNU Affero General Public License
 *    along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the GNU Affero General Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 */

#define MONGO_LOG_DEFAULT_COMPONENT ::mongo::logger::LogComponent::kStorage

#include "mongo/platform/basic.h"

#include "../rocks_index.h"

#include <cstdlib>
#include <memory>
#include <sstream>
#include <string>
#include <vector>

#include <rocksdb/db.h>
#include <rocksdb/iterator.h>
#include <rocksdb/utilities/write_batch_with_index.h>

#include "mongo/base/checked_cast.h"
#include "mongo/bson/bsonobjbuilder.h"
#include "mongo/db/concurrency/write_conflict_exception.h"
#include "mongo/db/storage/index_entry_comparison.h"
#include "mongo/s/stale_exception.h"
#include "mongo/stdx/memory.h"
#include "mongo/util/log.h"
#include "mongo/util/mongoutils/str.h"

#include "../rocks_engine.h"
#include "ChunkRocksDBInstance.h"
#include "PartitionedRocksEngine.h"
#include "chunk_rocks_index.h"

namespace mongo {

    namespace {

        class IndexValueList {
        protected:
            rocksdb::Slice slice;

            static void Append(const rocksdb::Slice& element, std::string& list) {
                uint16_t valueSize = element.size();
                list.append((char*)&valueSize, sizeof(uint16_t));
                list.append(element.data(), element.size());
            }

            static uint16_t GetValueSize(const char* block) { return *(uint16_t*)block; }

            static uint16_t GetBlockSize(const char* block) {
                return sizeof(uint16_t) + GetValueSize(block);
            }

            static const char* GetValue(const char* block) { return block + sizeof(uint16_t); }

        public:
            class Iterator : public std::iterator<std::forward_iterator_tag, rocksdb::Slice> {
            protected:
                const char* current;
                const char* end;  //  Keep end for consistency check

                Iterator(const char* start, const char* end) : current(start), end(end) {}

            public:
                Iterator(const Iterator& it) : Iterator(it.current, it.end) {}

                Iterator& operator=(const Iterator& it) {
                    current = it.current;
                    end = it.end;
                    return *this;
                }

                Iterator& operator++()  //  Prefix increment: ++i
                {
                    current += GetBlockSize(current);
                    invariant(current <= end);
                    return *this;
                }

                Iterator operator++(int)  //  Postfix increment: i++
                {
                    Iterator it(*this);
                    operator++();
                    return it;
                }

                bool operator==(const Iterator& rhs) const { return current == rhs.current; }

                bool operator!=(const Iterator& rhs) const { return current != rhs.current; }

                rocksdb::Slice operator*() const {
                    invariant(current < end);
                    return rocksdb::Slice(current + sizeof(uint16_t), GetValueSize(current));
                }

                friend class IndexValueList;
            };

            static Status CheckValue(const rocksdb::Slice& value) {
                if (value.size() >= std::numeric_limits<uint16_t>::max())
                    MDB_RET(Status(ErrorCodes::KeyTooLong,
                                   "Value generated by IndexRecordFormatter is too long"));

                return Status::OK();
            }

            Iterator begin() const { return Iterator(slice.data(), slice.data() + slice.size()); }

            Iterator end() const {
                return Iterator(slice.data() + slice.size(), slice.data() + slice.size());
            }

            IndexValueList(const rocksdb::Slice& listSlice) : slice(listSlice) {}

            Iterator Find(const rocksdb::Slice& element) const {
                return std::find(begin(), end(), element);
            }

            bool HasElement(const rocksdb::Slice& element) const { return Find(element) != end(); }

            std::string RemoveElement(Iterator it) const {
                size_t partToRemoveSize = sizeof(uint16_t) + (*it).size();
                if (partToRemoveSize == slice.size()) return std::string();

                std::string newValue;
                newValue.reserve(slice.size() - partToRemoveSize);
                newValue.append(slice.data(), (size_t)(it.current - slice.data()));

                it++;
                newValue.append(it.current, (size_t)(slice.data() + slice.size() - it.current));

                return newValue;
            }

            std::string AddElement(const rocksdb::Slice& elementToAdd) const {
                std::string result;
                result.reserve(slice.size() + elementToAdd.size() + sizeof(uint64_t));
                result.append(slice.data(), slice.size());
                Append(elementToAdd, result);
                return result;
            }

            static Status CreateList(const std::vector<std::string>& elements,
                                     std::string& listToCreate) {
                size_t totalValueSize = 0;
                for (const std::string& element : elements) {
                    MDB_RIF(CheckValue(element));
                    totalValueSize += element.size() + sizeof(uint16_t);
                }

                //  Copy all values with their sizes into single value
                listToCreate.clear();
                listToCreate.reserve(totalValueSize);

                for (const std::string& element : elements) Append(element, listToCreate);

                return Status::OK();
            }

            static Status CreateList(const rocksdb::Slice& singleElement,
                                     std::string& listToCreate) {
                MDB_RIF(CheckValue(singleElement));

                listToCreate.clear();
                listToCreate.reserve(sizeof(uint16_t) + singleElement.size());

                Append(singleElement, listToCreate);
                return Status::OK();
            }

            static rocksdb::Slice GetFirstValue(const rocksdb::Slice& list) {
                invariant(list.size() > 2);
                invariant(GetBlockSize(list.data()) <= list.size());
                return rocksdb::Slice(GetValue(list.data()), GetValueSize(list.data()));
            }
        };
    }

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //
    //  ChunkRocksCursor is heavily based on RocksCursorBase. The difference is that we rely on
    //  IIndexRecordFormatter
    //  to format keys for us. It allows us to have single class for both unique and non-unique
    //  (standard) indexes.
    //
    //  The logic is: have an iterator which chains current write/update batch from recovery unit
    //  and iterator from
    //  database. Use IIndexRecordFormatter to create and parse keys.
    //
    class ChunkRocksCursor : public SortedDataInterface::Cursor {
    public:
        ChunkRocksCursor(OperationContext* txn, const ChunkRocksIndexBase& indexer, bool forward)
            : _indexer(indexer), _forward(forward), _txn(txn) {
            auto snapshot = RocksRecoveryUnit::getRocksRecoveryUnit(txn)->snapshot();
            if (snapshot) {
                _currentSequenceNumber = snapshot->GetSequenceNumber();
            }
        }

        boost::optional<IndexKeyEntry> next(RequestedInfo parts) override {
            if (_eof) return {};

            if (!_lastMoveWasRestore) advanceCursor();

            updatePosition();

            return curr(parts);
        }

        void setEndPosition(const BSONObj& key, bool inclusive) override {
            _endPosition.clear();
            if (key.isEmpty()) return;

            const auto discriminator =
                _forward == inclusive ? KeyString::kExclusiveAfter : KeyString::kExclusiveBefore;
            GetFormatter().GenerateCursorPosition(stripFieldNames(key), discriminator,
                                                  _endPosition);
        }

        boost::optional<IndexKeyEntry> seek(const BSONObj& key, bool inclusive,
                                            RequestedInfo parts) override {
            const BSONObj keyWithoutFieldNames = stripFieldNames(key);
            const auto discriminator =
                _forward == inclusive ? KeyString::kExclusiveBefore : KeyString::kExclusiveAfter;

            seekQuery(keyWithoutFieldNames, discriminator);
            return curr(parts);
        }

        boost::optional<IndexKeyEntry> seek(const IndexSeekPoint& seekPoint,
                                            RequestedInfo parts) override {
            // Make a key representing the location to which we want to advance.
            BSONObj key = IndexEntryComparison::makeQueryObject(seekPoint, _forward);

            // makeQueryObject handles the discriminator in the real exclusive cases.
            const auto discriminator =
                _forward ? KeyString::kExclusiveBefore : KeyString::kExclusiveAfter;

            seekQuery(key, discriminator);
            return curr(parts);
        }

        boost::optional<IndexKeyEntry> seekExact(const BSONObj& key, RequestedInfo parts) override {
            _eof = false;
            _iterator.reset();

            KeyString query(KeyString::Version::V1);
            std::string prefixedKey(GetPrefix());
            query.resetToKey(stripFieldNames(key), GetFormatter().GetOrdering());
            _query = query.toString();
            prefixedKey.append(query.getBuffer(), query.getSize());
            _key = prefixedKey;

            _value.clear();

            rocksdb::Status status = RocksRecoveryUnit::getRocksRecoveryUnit(_txn)->Get(
                prefixedKey, &_value, _indexer.GetColumnFamily());

            if (status.IsNotFound()) {
                _eof = true;
            } else if (!status.ok()) {
                rocksdb_err_handle(status);
            }
            updatePosition();
            return curr(parts);
        }

        void save() override {
            if (!_lastMoveWasRestore) _savedEOF = _eof;
        }

        void saveUnpositioned() override { _savedEOF = true; }

        void restore() override {
            _indexer.getDBInstance()->GetRocksRecoveryUnit(_txn);
            auto ru = RocksRecoveryUnit::getRocksRecoveryUnit(_txn);
            auto snapshot = ru->snapshot();
            if (!snapshot) {
                return;
            }
            if (_iterator && _currentSequenceNumber == snapshot->GetSequenceNumber()) return;

            ResetIterator();
            _currentSequenceNumber = snapshot->GetSequenceNumber();

            if (!_savedEOF) _lastMoveWasRestore = !seekCursor(_key);
        }

        void detachFromOperationContext() {
            _txn = nullptr;
            _iterator.reset();
        }

        void reattachToOperationContext(OperationContext* txn) {
            _txn = txn;  // iterator recreated in restore()
            _indexer.getDBInstance()->GetRocksRecoveryUnit(_txn);
        }

    protected:
        const std::string& GetPrefix() const { return GetFormatter().GetPrefix(); }

        boost::optional<IndexKeyEntry> curr(RequestedInfo parts) {
            if (_eof) return {};

            if (!_indexer.getDBInstance()->isSystemChunk()) {
                auto cf_des =
                    _indexer.getDBInstance()->getColumnFamilyDescriptionInPreperSplit("index");
                if (!cf_des) {
                    return {};  // cf_des can't be null.
                }
                if (/*_indexer.getDBInstance()->bockingReadIOofRight() && */
                    !cf_des->DoesRecordBelongToChunk(keySlice(), valueSlice())) {
                    NamespaceString nss =
                        _indexer.getDBInstance()->GetMetadata().GetCollection().getNs();
                    index_LOG(3) << "ChunkRocksCursor::curr()-> belong right db ns: " << nss;
                    mongo::ChunkVersion received;
                    mongo::ChunkVersion wanted;
                    _indexer.getDBInstance()->GetRocksRecoveryUnit(_txn)->getChunkVersion(
                        _txn, _indexer.getDBInstance(), received, wanted);

                    throw mongo::SendStaleConfigException(
                        nss.ns(),
                        str::stream() << "chunk " << nss.ns() << " chunk version not equal:",
                        received, wanted);
                }
            }

            BSONObj keyBSON;
            RecordId recordId;

            invariantRocksOK(GetFormatter().ParseRecord(std::make_pair(keySlice(), valueSlice()),
                                                        (parts & kWantKey) ? &keyBSON : nullptr,
                                                        &recordId));

            return {{std::move(keyBSON), recordId}};
        }

        void advanceCursor() {
            if (_eof) return;

            if (!_iterator) {
                ResetIterator();
                _iterator->SeekPrefix(_key);
                // advanceCursor() should only ever be called in states where the above seek
                // will succeed in finding the exact key
                invariant(_iterator->Valid());
            }

            if (_forward)
                _iterator->Next();
            else
                _iterator->Prev();

            _updateOnIteratorValidity();
        }

        void seekQuery(const BSONObj& key, KeyString::Discriminator discriminator) {
            _query.clear();
            invariantRocksOK(GetFormatter().GenerateCursorPosition(key, discriminator, _query));
            seekCursor(_query);
            updatePosition();
        }

        // Seeks to query. Returns true on exact match.
        bool seekCursor(const rocksdb::Slice& keySlice) {
            rocksdb::Slice querySlice = AdjustQuery(keySlice);
            auto iter = iterator();
            iter->Seek(querySlice);
            if (!_updateOnIteratorValidity()) {
                if (!_forward) {
                    // this will give lower bound behavior for backwards
                    iter->SeekToLast();
                    _updateOnIteratorValidity();
                }
                return false;
            }

            if (iter->key() == querySlice) return true;

            if (!_forward) {
                // if we can't find the exact result going backwards, we
                // need to call Prev() so that we're at the first value
                // less than (to the left of) what we were searching for,
                // rather than the first value greater than (to the right
                // of) the value we were searching for.
                iter->Prev();
                _updateOnIteratorValidity();
            }

            return false;
        }

        void updatePosition() {
            _lastMoveWasRestore = false;
            if (_eof) return;

            if (!_iterator) {
                // _iterator may be null, because for unique index it's possible that we just run
                // seekExact
                // and get value direcly from recovery unit without using iterator. This is how
                // RocksUniqueIndex
                // is working. However currently we don't support it and leave the code just in
                // case.
                //_key = _query;
            } else {
                //  Copy key in case we want to recover further. TODO: consider just always
                //  rely on _iterator->key()
                rocksdb::Slice key = AdjustKey(_iterator->key());
                _key = std::string(key.data(), key.size());
            }

            if (!_endPosition.empty()) {
                int cmp = _key.compare(_endPosition);
                if (_forward ? cmp > 0 : cmp < 0) {
                    _eof = true;
                    return;
                }
            }
        }

        // ensure that _iterator is initialized and return a pointer to it
        RocksIterator* iterator() {
            if (!_iterator) ResetIterator();

            return _iterator.get();
        }

        void ResetIterator() {
            _iterator.reset(RocksRecoveryUnit::getRocksRecoveryUnit(_txn)->NewIterator(
                GetPrefix(), false, _indexer.GetColumnFamily()));
        }

        // Update _eof based on _iterator->Valid() and return _iterator->Valid()
        bool _updateOnIteratorValidity() {
            if (_iterator->Valid()) {
                _eof = false;
                return true;
            }

            _eof = true;
            invariantRocksOK(_iterator->status());
            return false;
        }

        rocksdb::Slice valueSlice() const { return _iterator ? _iterator->value() : _value; }

        rocksdb::Slice keySlice() const { return rocksdb::Slice(_key); }

        bool isUniqueIndex() const { return GetFormatter().IsUnique(); }

        const IIndexRecordFormatter& GetFormatter() const {
            return _indexer.GetIndexRecordFormatter();
        }

        //
        //  TODO: currently we use PrefixStrippingIterator, which is default iterator for
        //  RocksRecoveryUnit
        //  We don't need prefix stripping since we handle prefixes by ourselves. We need to replace
        //  this
        //  iterator with other and then we can remove AdjustQuery and AdjustKey.
        //
        rocksdb::Slice AdjustQuery(const rocksdb::Slice& query) {
            rocksdb::Slice result = query;
            result.remove_prefix(GetPrefix().size());
            return result;
        }

        rocksdb::Slice AdjustKey(const rocksdb::Slice& key) {
            //  Ugly hack here (will remove later), is based on fact that PrefixStrippingIterator
            //  strips prefix
            return rocksdb::Slice(key.data() - GetPrefix().size(), GetPrefix().size() + key.size());
        }

        const ChunkRocksIndexBase& _indexer;
        std::unique_ptr<RocksIterator> _iterator;
        const bool _forward;
        bool _lastMoveWasRestore = false;

        // These are for storing savePosition/restorePosition state
        bool _savedEOF = false;
        rocksdb::SequenceNumber _currentSequenceNumber = 0;

        std::string _query;  //  Current position to seek
        std::string _key;    //  Latest key, required for recovery case
        std::string _endPosition;

        bool _eof = false;
        OperationContext* _txn;

        // In case we get value from stores the value associated with the latest call to seekExact()
        std::string _value;
    };  //  ChunkRocksCursor
    ///////////////////////////////////////////////////////////////////////////////////////////////////

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //
    //  Bulk builder for a non-unique index
    //
    class ChunkStandardBulkBuilder : public SortedDataBuilderInterface {
    public:
        ChunkStandardBulkBuilder(ChunkRocksStandardIndex& index, OperationContext* txn)
            : _index(index), _txn(txn) {}

        Status addKey(const BSONObj& key, const RecordId& loc) {
            return _index.insert(_txn, key, loc, true);
        }

        void commit(bool mayInterrupt) {
            WriteUnitOfWork uow(_txn);
            uow.commit();
        }

    private:
        ChunkRocksStandardIndex& _index;
        OperationContext* _txn;
    };  //  StandardBulkBuilder
    ///////////////////////////////////////////////////////////////////////////////////////////////////

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //  Bulk builds a unique index.
    //
    //  In order to support unique indexes in dupsAllowed mode this class only does an actual insert
    //  after it sees a key after the one we are trying to insert. This allows us to gather up all
    //  duplicate locs and insert them all together. This is necessary since bulk cursors can only
    //  append data.
    //
    class ChunkRocksUniqueBulkBuilder : public SortedDataBuilderInterface {
    public:
        ChunkRocksUniqueBulkBuilder(ChunkRocksUniqueIndex& indexer, OperationContext* txn,
                                    bool dupsAllowed)
            : _indexer(indexer), _txn(txn), _dupsAllowed(dupsAllowed) {}

        Status addKey(const BSONObj& newKey, const RecordId& loc) {
            MDB_RIF(checkKeySize(newKey));

            std::pair<std::string, std::string> recordKeyValuePair;
            MDB_RIF(
                _indexer.GetIndexRecordFormatter().GenerateRecord(newKey, loc, recordKeyValuePair));

            if (_key != recordKeyValuePair.first) {
                if (!_key.empty()) doInsert();

                invariant(_valuesForTheSameKey.empty());
            } else {
                if (!_dupsAllowed)  //  Key is duplicated
                    MDB_RET(Status(ErrorCodes::DuplicateKey, dupKeyError(newKey)));

                // If we get here, we are in the weird mode where dups are allowed on a unique
                // index, so add ourselves to the list of duplicate locs. This also replaces the
                // _key which is correct since any dups seen later are likely to be newer.
            }

            _key = std::move(recordKeyValuePair.first);
            _valuesForTheSameKey.push_back(std::move(recordKeyValuePair.second));

            return Status::OK();
        }

        void commit(bool mayInterrupt) {
            WriteUnitOfWork uow(_txn);
            if (!_valuesForTheSameKey.empty()) doInsert();  //  Insert the last unique key

            uow.commit();
        }

    private:
        void doInsert() {
            invariant(!_valuesForTheSameKey.empty());

            std::string value;
            invariantOK(IndexValueList::CreateList(_valuesForTheSameKey, value));

            auto ru = RocksRecoveryUnit::getRocksRecoveryUnit(_txn);
            ru->writeBatch()->Put(_indexer.GetColumnFamily(), _key, value);

            _valuesForTheSameKey.clear();
        }

        ChunkRocksUniqueIndex& _indexer;
        OperationContext* _txn;
        const bool _dupsAllowed;

        std::string _key;
        std::vector<std::string> _valuesForTheSameKey;
    };  //  ChunkRocksUniqueBulkBuilder
    ///////////////////////////////////////////////////////////////////////////////////////////////////

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    // RocksIndexBase
    ChunkRocksIndexBase::ChunkRocksIndexBase(mongo::ChunkRocksDBInstance& db,
                                             std::unique_ptr<IIndexRecordFormatter> recordFormatter)
        : RocksIndexBase(&db), _recordFormatter(std::move(recordFormatter)) {
        auto prefiSlice = rocksdb::Slice(_recordFormatter->GetPrefix());
        uint32_t prefix = decodePrefix(prefiSlice);
        index_LOG(1) << "ChunkRocksIndexBase::ChunkRocksIndexBase()-> prefix: "
                     << _recordFormatter->GetPrefix() << "prefix: " << prefix;

        uint64_t storageSize;  //  TODO: Formatter need to support logic without using prefixes
        std::string nextPrefix = rocksGetNextPrefix(_recordFormatter->GetPrefix());
        rocksdb::Range wholeRange(_recordFormatter->GetPrefix(), nextPrefix);
        getDB()->GetApproximateSizes(GetColumnFamily(), &wholeRange, 1, &storageSize);
        _indexStorageSize.store(static_cast<long long>(storageSize), std::memory_order_relaxed);
    }

    bool ChunkRocksIndexBase::isEmpty(OperationContext* txn) {
        auto ru = _db->GetRocksRecoveryUnit(txn);
        std::unique_ptr<rocksdb::Iterator> it(ru->NewIterator(_recordFormatter->GetPrefix()));
        it->SeekToFirst();
        return !it->Valid();
    }

    rocksdb::ColumnFamilyHandle* ChunkRocksIndexBase::GetColumnFamily() const {
        return _db->GetColumnFamily(_recordFormatter->GetColumnFamilyId());
    }

    Status ChunkRocksIndexBase::GenerateRecord(
        const BSONObj& key, const RecordId& loc,
        std::pair<std::string, std::string>& recordKeyValuePair) {
        MDB_RET(_recordFormatter->GenerateRecord(key, loc, recordKeyValuePair));
    }

    Status ChunkRocksIndexBase::PrepareWrite(
        OperationContext* txn, const BSONObj& key, const RecordId& loc,
        std::pair<std::string, std::string>& recordKeyValuePair) {
        MDB_RIF(checkKeySize(key));

        MDB_RIF(GenerateRecord(key, loc, recordKeyValuePair));

        //  Check whether write for this key was already registered
        //  This should work fine for 2 column family since we use different prefixes
        if (nullptr == _db->GetRocksRecoveryUnit(txn)->transaction()) {
            throw WriteConflictException();  // can't be null
        }
        if (!_db->GetRocksRecoveryUnit(txn)->transaction()->registerWrite(recordKeyValuePair.first))
            throw WriteConflictException();

        return Status::OK();
    }

    Status ChunkRocksIndexBase::insert(OperationContext* txn, const BSONObj& key,
                                       const RecordId& loc, bool dupsAllowed) {
        std::pair<std::string, std::string> recordKeyValuePair;
        MDB_RIF(PrepareWrite(txn, key, loc, recordKeyValuePair));

        // MDB_RIF(onInsert(txn, dupsAllowed, recordKeyValuePair));
        Status st = onInsert(txn, dupsAllowed, recordKeyValuePair);
        if (!st.isOK()) {
            if (st.code() == ErrorCodes::DuplicateKey) {
                return Status(ErrorCodes::DuplicateKey, dupKeyError(key));
            } else {
                return st;
            }
        }
        if (recordKeyValuePair.first.empty() ||
            recordKeyValuePair.second.empty())  //  Don't need to add anything
            return Status::OK();

        _indexStorageSize.fetch_add(recordKeyValuePair.first.size(), std::memory_order_relaxed);

        _db->GetRocksRecoveryUnit(txn)->writeBatch()->Put(
            GetColumnFamily(), recordKeyValuePair.first, recordKeyValuePair.second);

        return Status::OK();
    }

    void ChunkRocksIndexBase::unindex(OperationContext* txn, const BSONObj& key,
                                      const RecordId& loc, bool dupsAllowed) {
        if (!checkKeySize(key).isOK()) {
            return;
        }

        std::pair<std::string, std::string> recordKeyValuePair;
        invariantOK(PrepareWrite(txn, key, loc, recordKeyValuePair));

        _indexStorageSize.fetch_sub(recordKeyValuePair.first.size(), std::memory_order_relaxed);

        invariantOK(finalizeUnindex(txn, recordKeyValuePair, dupsAllowed));
    }

    std::unique_ptr<SortedDataInterface::Cursor> ChunkRocksIndexBase::newCursor(
        OperationContext* txn, bool forward) const {
        _db->GetRocksRecoveryUnit(txn);
        return std::unique_ptr<SortedDataInterface::Cursor>(
            new ChunkRocksCursor(txn, *this, forward));
    }

    IIndexRecordFormatter& ChunkRocksIndexBase::GetIndexRecordFormatter() const {
        return *_recordFormatter.get();
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////////

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //
    // ChunkRocksUniqueIndex::RocksUniqueIndex
    //
    ChunkRocksUniqueIndex::ChunkRocksUniqueIndex(
        ChunkRocksDBInstance& db, std::unique_ptr<IIndexRecordFormatter> recordFormatter)
        : ChunkRocksIndexBase(db, std::move(recordFormatter)) {}

    Status ChunkRocksUniqueIndex::onInsert(
        OperationContext* txn, bool dupsAllowed,
        std::pair<std::string, std::string>& recordKeyValuePair) {
        auto ru = _db->GetRocksRecoveryUnit(txn);

        //  Check whether we have dublications for this key
        std::string valueInIndex;
        auto getStatus = ru->Get(recordKeyValuePair.first, &valueInIndex, GetColumnFamily());

        if (getStatus.IsNotFound()) {
            std::string newValue;
            MDB_RIF(IndexValueList::CreateList(recordKeyValuePair.second, newValue));
            recordKeyValuePair.second = newValue;

            return Status::OK();
        }

        MDB_RIF(getStatus);

        MDB_RIF(IndexValueList::CheckValue(recordKeyValuePair.second));

        //  We already have a record for such key.
        //  It could be because we are adding the same record second time.
        //  Unfortunatelly it also can happen because dublications for Unique index are allowed when
        //  dupsAllowed = true
        //  This dublications should be resolved before the end of transaction, but during the
        //  transaction we should be
        //  able to keep all RecordIds for the same key.
        //
        //  To be able to distinguish RecordIds related to different keys, we prefix value with it's
        //  size.
        //  So our values have the following format: (valueSize + value)
        IndexValueList listOfValuesInIndex(valueInIndex);
        if (listOfValuesInIndex.HasElement(recordKeyValuePair.second)) {
            if (_recordFormatter
                    ->IsLocalKey())  //  Special hack for the case when we have _id duplication
                return Status(ErrorCodes::DuplicateKey, dupKeyError(BSON("id"
                                                                         << "local_key")));
            // MDB_RET(Status(ErrorCodes::DuplicateKey, dupKeyError(BSON("id"
            //                                                          << "local_key"))));

            recordKeyValuePair.first = "";
            recordKeyValuePair.second = "";

            return Status::OK();
        }

        if (!dupsAllowed) MDB_RET(Status(ErrorCodes::DuplicateKey, "Duplicated key"));

        recordKeyValuePair.second = listOfValuesInIndex.AddElement(recordKeyValuePair.second);

        return Status::OK();
    }

    Status ChunkRocksUniqueIndex::finalizeUnindex(
        OperationContext* txn, const std::pair<std::string, std::string>& recordKeyValuePair,
        bool dupsAllowed) {
        auto ru = _db->GetRocksRecoveryUnit(txn);
        if (!dupsAllowed) {
            ru->writeBatch()->Delete(GetColumnFamily(), recordKeyValuePair.first);
            return Status::OK();
        }

        std::string valueInIndex;
        auto getStatus = ru->Get(recordKeyValuePair.first, &valueInIndex, GetColumnFamily());
        if (getStatus.IsNotFound()) return Status::OK();
        MDB_RIF(getStatus);

        IndexValueList valuesList(valueInIndex);
        auto valuePosition = valuesList.Find(recordKeyValuePair.second);
        if (valuePosition == valuesList.end()) {
            warning().stream() << "value {" + recordKeyValuePair.second
                               << "} not found in the index for key {" << recordKeyValuePair.first
                               << "}";
            return Status::OK();
        }

        std::string newValue = valuesList.RemoveElement(valuePosition);

        if (newValue.empty())  //  No more values in the list
            ru->writeBatch()->Delete(GetColumnFamily(), recordKeyValuePair.first);
        else
            ru->writeBatch()->Put(GetColumnFamily(), recordKeyValuePair.first, newValue);

        return Status::OK();
    }

    Status ChunkRocksUniqueIndex::dupKeyCheck(OperationContext* txn, const BSONObj& key,
                                              const RecordId& loc) {
        std::pair<std::string, std::string> recordKeyValuePair;
        MDB_RIF(GenerateRecord(key, loc, recordKeyValuePair));

        std::string valueListInIndex;
        auto ru = _db->GetRocksRecoveryUnit(txn);
        auto getStatus = ru->Get(recordKeyValuePair.first, &valueListInIndex, GetColumnFamily());
        if (getStatus.IsNotFound()) return Status::OK();
        MDB_RIF(getStatus);

        if (IndexValueList(valueListInIndex).HasElement(recordKeyValuePair.second))
            return Status::OK();

        return Status(ErrorCodes::DuplicateKey, dupKeyError(key));
    }

    SortedDataBuilderInterface* ChunkRocksUniqueIndex::getBulkBuilder(OperationContext* txn,
                                                                      bool dupsAllowed) {
        SetCorrectDBForRecoveryUnit(txn);
        return new ChunkRocksUniqueBulkBuilder(*this, txn, dupsAllowed);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////////

    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //
    // ChunkRocksStandardIndex
    //
    ChunkRocksStandardIndex::ChunkRocksStandardIndex(
        mongo::ChunkRocksDBInstance& db, std::unique_ptr<IIndexRecordFormatter> recordFormatter)
        : ChunkRocksIndexBase(db, std::move(recordFormatter)) {}

    Status ChunkRocksStandardIndex::onInsert(
        OperationContext* txn, bool dupsAllowed,
        std::pair<std::string, std::string>& recordKeyValuePair) {
        invariant(dupsAllowed);  //  For non-unique index dups should allways be allowed
        return Status::OK();
    }

    Status ChunkRocksStandardIndex::finalizeUnindex(
        OperationContext* txn, const std::pair<std::string, std::string>& recordKeyValuePair,
        bool dupsAllowed) {
        invariant(dupsAllowed);  //  For non-unique index dups should always be allowed
        _db->GetRocksRecoveryUnit(txn)->writeBatch()->Delete(GetColumnFamily(),
                                                             recordKeyValuePair.first);
        return Status::OK();
    }

    SortedDataBuilderInterface* ChunkRocksStandardIndex::getBulkBuilder(OperationContext* txn,
                                                                        bool dupsAllowed) {
        invariant(dupsAllowed);
        SetCorrectDBForRecoveryUnit(txn);
        return new ChunkStandardBulkBuilder(*this, txn);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////////

}  // namespace mongo
